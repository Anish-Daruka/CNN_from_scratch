{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317d349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'after each layer->\\n    output = sum (inputs * weights) + bias\\n    output = activation(output)\\n   calculating loss fn\\n   backward propagation->calculating gradients\\n   optimization->gradient descent\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"after each layer->\n",
    "    output = sum (inputs * weights) + bias\n",
    "    output = activation(output)\n",
    "   calculating loss fn\n",
    "   backward propagation->calculating gradients\n",
    "   optimization->gradient descent\n",
    "\"\"\"\n",
    "# numpy array->as a vector,python list->need loops\n",
    "hyperparameters_for_cnnlayer = {\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 32,\n",
    "    'kernel':3,\n",
    "    'padding': 0,\n",
    "    'stride': 1\n",
    "}\n",
    "\"\"\"model_structure->\n",
    "28*28->26*26(4 filters)->ReLU->13*13(max_pooling)->11*11(8 filters)->ReLU->5*5(max_pooling)->linear(8*5*5)->10\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5ca5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0691149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the datasets from mnist custom data in a regular manner\n",
    "train = torchvision.datasets.MNIST(root='./data', train=True, download=False, transform=ToTensor())\n",
    "test = torchvision.datasets.MNIST(root='./data', train=False, download=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e7c3b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.2        0.62352941 0.99215686\n",
      "  0.62352941 0.19607843 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.18823529 0.93333333 0.98823529 0.98823529\n",
      "  0.98823529 0.92941176 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.21176471 0.89019608 0.99215686 0.98823529 0.9372549\n",
      "  0.91372549 0.98823529 0.22352941 0.02352941 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.03921569\n",
      "  0.23529412 0.87843137 0.98823529 0.99215686 0.98823529 0.79215686\n",
      "  0.32941176 0.98823529 0.99215686 0.47843137 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.63921569\n",
      "  0.98823529 0.98823529 0.98823529 0.99215686 0.98823529 0.98823529\n",
      "  0.37647059 0.74117647 0.99215686 0.65490196 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.93333333\n",
      "  0.99215686 0.99215686 0.74509804 0.44705882 0.99215686 0.89411765\n",
      "  0.18431373 0.30980392 1.         0.65882353 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.18823529 0.93333333 0.98823529\n",
      "  0.98823529 0.70196078 0.04705882 0.29411765 0.4745098  0.08235294\n",
      "  0.         0.         0.99215686 0.95294118 0.19607843 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14901961 0.64705882 0.99215686 0.91372549\n",
      "  0.81568627 0.32941176 0.         0.         0.         0.\n",
      "  0.         0.         0.99215686 0.98823529 0.64705882 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.02745098 0.69803922 0.98823529 0.94117647 0.27843137\n",
      "  0.0745098  0.10980392 0.         0.         0.         0.\n",
      "  0.         0.         0.99215686 0.98823529 0.76470588 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.22352941 0.98823529 0.98823529 0.24705882 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.99215686 0.98823529 0.76470588 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.77647059 0.99215686 0.74509804 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.99215686 0.76862745 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.29803922 0.96470588 0.98823529 0.43921569 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.99215686 0.98823529 0.58039216 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.90196078 0.09803922 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02745098 0.52941176 0.99215686 0.72941176 0.04705882 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.8745098  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.02745098\n",
      "  0.51372549 0.98823529 0.88235294 0.27843137 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.56862745 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.18823529 0.64705882\n",
      "  0.98823529 0.67843137 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.3372549  0.99215686 0.88235294 0.         0.         0.\n",
      "  0.         0.         0.         0.44705882 0.93333333 0.99215686\n",
      "  0.63529412 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.97647059 0.57254902 0.18823529 0.11372549\n",
      "  0.33333333 0.69803922 0.88235294 0.99215686 0.8745098  0.65490196\n",
      "  0.21960784 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.98823529 0.98823529 0.89803922 0.84313725\n",
      "  0.98823529 0.98823529 0.98823529 0.76862745 0.50980392 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.10980392 0.78039216 0.98823529 0.98823529 0.99215686 0.98823529\n",
      "  0.98823529 0.91372549 0.56862745 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.09803922 0.50196078 0.98823529 0.99215686 0.98823529\n",
      "  0.55294118 0.14509804 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "(60000,)\n",
      "5\n",
      "['0 - zero' '1 - one' '2 - two' '3 - three' '4 - four' '5 - five'\n",
      " '6 - six' '7 - seven' '8 - eight' '9 - nine']\n",
      "(10000, 28, 28)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.45490196 0.49019608\n",
      "  0.67058824 1.         1.         0.58823529 0.36470588 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.6627451  0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.99215686 0.85490196 0.11764706\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.6627451  0.99215686 0.99215686 0.99215686\n",
      "  0.83529412 0.55686275 0.69019608 0.99215686 0.99215686 0.47843137\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.20392157 0.98039216 0.99215686 0.82352941 0.1254902\n",
      "  0.04705882 0.         0.02352941 0.80784314 0.99215686 0.54901961\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30196078 0.98431373 0.82352941 0.09803922 0.\n",
      "  0.         0.         0.47843137 0.97254902 0.99215686 0.25490196\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.12156863 0.07058824 0.         0.\n",
      "  0.         0.         0.81960784 0.99215686 0.99215686 0.25490196\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.45882353 0.96862745 0.99215686 0.77647059 0.03921569\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.29803922 0.96862745 0.99215686 0.90588235 0.24705882 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.50196078 0.99215686 0.99215686 0.56470588 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.69019608\n",
      "  0.96470588 0.99215686 0.62352941 0.04705882 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.09803922 0.91764706\n",
      "  0.99215686 0.91372549 0.1372549  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.77647059 0.99215686\n",
      "  0.99215686 0.55294118 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.30588235 0.97254902 0.99215686\n",
      "  0.74117647 0.04705882 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.0745098  0.78431373 0.99215686 0.99215686\n",
      "  0.55294118 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5254902  0.99215686 0.99215686 0.67843137\n",
      "  0.04705882 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.97254902 0.99215686 0.99215686 0.09803922\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.97254902 0.99215686 0.99215686 0.16862745\n",
      "  0.07843137 0.07843137 0.07843137 0.07843137 0.01960784 0.\n",
      "  0.01960784 0.07843137 0.07843137 0.14509804 0.58823529 0.58823529\n",
      "  0.58823529 0.57647059 0.03921569 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.97254902 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.99215686 0.65882353 0.56078431\n",
      "  0.65098039 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.48235294 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.68235294 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.97647059 0.96862745 0.96862745 0.6627451\n",
      "  0.45882353 0.45882353 0.22352941 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.4627451  0.48235294 0.48235294\n",
      "  0.48235294 0.65098039 0.99215686 0.99215686 0.99215686 0.60784314\n",
      "  0.48235294 0.48235294 0.16078431 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "(10000,)\n",
      "7\n",
      "['0 - zero' '1 - one' '2 - two' '3 - three' '4 - four' '5 - five'\n",
      " '6 - six' '7 - seven' '8 - eight' '9 - nine']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pk/lwrryr955rv0c9bcz4l5tcq00000gn/T/ipykernel_5000/502980871.py:3: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  train_data = np.array(train.data)/255 # convert to numpy array and normalize\n",
      "/var/folders/pk/lwrryr955rv0c9bcz4l5tcq00000gn/T/ipykernel_5000/502980871.py:6: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  train_value = np.array(train.targets) # convert to numpy array\n",
      "/var/folders/pk/lwrryr955rv0c9bcz4l5tcq00000gn/T/ipykernel_5000/502980871.py:13: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  test_data = np.array(test.data)/255 # convert to numpy array and normalize\n",
      "/var/folders/pk/lwrryr955rv0c9bcz4l5tcq00000gn/T/ipykernel_5000/502980871.py:16: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  test_value = np.array(test.targets) # convert to numpy array\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data = np.array(train.data)/255 # convert to numpy array and normalize\n",
    "print(train_data.shape)\n",
    "print(train_data[1])\n",
    "train_value = np.array(train.targets) # convert to numpy array\n",
    "print(train_value.shape)\n",
    "print(train_value[0])\n",
    "train_classes=classes= np.array(train.classes)\n",
    "print(train_classes)\n",
    "\n",
    "\n",
    "test_data = np.array(test.data)/255 # convert to numpy array and normalize\n",
    "print(test_data.shape)\n",
    "print(test_data[1])\n",
    "test_value = np.array(test.targets) # convert to numpy array\n",
    "print(test_value.shape)\n",
    "print(test_value[0])\n",
    "test_classes = classes = np.array(test.classes)\n",
    "print(test_classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d356ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape: (60000, 1, 28, 28)\n",
      "test_data shape: (10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Add a new axis for features (channels) to train_data and test_data\n",
    "# Shape changes from (N, 28, 28) to (N, 1, 28, 28)\n",
    "train_data = train_data[:, np.newaxis, :, :]\n",
    "test_data = test_data[:, np.newaxis, :, :]\n",
    "print(\"train_data shape:\", train_data.shape)\n",
    "print(\"test_data shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a8c3a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20 47 74]\n",
      "[[1 2 0]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(np.array([2,3,4]),np.array([[1,2,3],[4,5,6],[7,8,9]]).T))\n",
    "a=np.array([[1,2,-3],[4,5,6],[7,8,9]])\n",
    "print(np.maximum(0,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec43f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating a neural network layer\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        print(\"here\")\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        print(\"here1\")\n",
    "        samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # print(\"y_pred:\",y_pred_clipped)\n",
    "\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # # If labels are one-hot encoded,\n",
    "        # # turn them into discrete values\n",
    "        # if len(y_true.shape) == 2:\n",
    "        #     y_true = np.argmax(y_true, axis=1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings\n",
    "    # Learning rate of 1.0 is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "\n",
    "\n",
    "optimizer=Optimizer_SGD(learning_rate=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2fc4601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating CNN layer\n",
    "\n",
    "class CNN_layer:\n",
    "    def __init__(self, in_channels=1, out_channels=1, kernel_size=3):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # Initialize filters: [out_channels, in_channels, kernel_size, kernel_size]\n",
    "        # Each output channel has its own set of filters for all input channels\n",
    "        self.filters = 0.1 * np.random.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        \n",
    "        # One bias per output channel\n",
    "        self.biases = 0.1 * np.random.randn(out_channels)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "       \n",
    "        self.input = input  \n",
    "\n",
    "        # Add batch dimension if missing\n",
    "\n",
    "        batch_size, channels, h, w = input.shape\n",
    "        assert channels == self.in_channels, f\"Expected {self.in_channels} channels, got {channels}\"\n",
    "\n",
    "        out_h = h - self.kernel_size + 1\n",
    "        out_w = w - self.kernel_size + 1\n",
    "\n",
    "        # Feature maps for each output channel and batch\n",
    "        self.output = np.zeros((batch_size, self.out_channels, out_h, out_w))\n",
    "\n",
    "        # Apply each output filter for each sample in batch\n",
    "        for b in range(batch_size):\n",
    "            for out_ch in range(self.out_channels):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        conv_sum = 0\n",
    "                        for in_ch in range(self.in_channels):\n",
    "                            patch = input[b, in_ch, i:i+self.kernel_size, j:j+self.kernel_size]\n",
    "                            conv_sum += np.sum(patch * self.filters[out_ch, in_ch])\n",
    "                        self.output[b, out_ch, i, j] = conv_sum + self.biases[out_ch]\n",
    "    \n",
    "    def backward(self, doutput):\n",
    "        input = self.input\n",
    "        batch_size, channels, h, w = input.shape\n",
    "        out_h = h - self.kernel_size + 1\n",
    "        out_w = w - self.kernel_size + 1\n",
    "\n",
    "        self.dfilters = np.zeros_like(self.filters)\n",
    "        self.dbiases = np.zeros_like(self.biases)\n",
    "        self.dinputs = np.zeros_like(input)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for out_ch in range(self.out_channels):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        for in_ch in range(self.in_channels):\n",
    "                            patch = input[b, in_ch, i:i+self.kernel_size, j:j+self.kernel_size]\n",
    "                            self.dfilters[out_ch, in_ch] += doutput[b, out_ch, i, j] * patch\n",
    "                            self.dinputs[b, in_ch, i:i+self.kernel_size, j:j+self.kernel_size] += doutput[b, out_ch, i, j] * self.filters[out_ch, in_ch]\n",
    "                            self.dbiases[out_ch] += doutput[b, out_ch, i, j]\n",
    "\n",
    "        # Remove batch dimension if input was single sample\n",
    "        if len(self.input.shape) == 3:\n",
    "            self.dinputs = self.dinputs[0]\n",
    "    \n",
    "    def optimize(self,learning_rate=0.01):\n",
    "        # Update filters and biases using gradients computed in backward pass\n",
    "        self.filters -= learning_rate * self.dfilters\n",
    "        self.biases -= learning_rate * self.dbiases\n",
    "\n",
    "\n",
    "\n",
    "class maxPool:\n",
    "    def __init__(self, kernel_size=2, stride=2):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        batch_size, channels, h, w = x.shape\n",
    "        out_h = (h - self.kernel_size) // self.stride + 1\n",
    "        out_w = (w - self.kernel_size) // self.stride + 1\n",
    "        self.output = np.zeros((batch_size, channels, out_h, out_w))\n",
    "        self.max_indices = np.zeros_like(self.output, dtype=int)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "                        patch = x[b, c, h_start:h_end, w_start:w_end]\n",
    "                        self.output[b, c, i, j] = np.max(patch)\n",
    "                        self.max_indices[b, c, i, j] = np.argmax(patch)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, doutput):\n",
    "        \"\"\"\n",
    "        doutput: gradient of loss w.r.t. output, same shape as self.output\n",
    "        \"\"\"\n",
    "        batch_size, channels, h, w = self.input.shape\n",
    "        self.dinputs = np.zeros_like(self.input)\n",
    "        out_h, out_w = doutput.shape[2], doutput.shape[3]\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "                        patch = self.input[b, c, h_start:h_end, w_start:w_end]\n",
    "                        max_idx = self.max_indices[b, c, i, j]\n",
    "                        idx = np.unravel_index(max_idx, patch.shape)\n",
    "                        self.dinputs[b, c, h_start:h_end, w_start:w_end][idx] += doutput[b, c, i, j]\n",
    "\n",
    "\n",
    "class flatten_layer:\n",
    "    def forward(self, x):\n",
    "        self.input_shape = x.shape\n",
    "        return x.reshape(x.shape[0], -1)    #batch_size, then all rest of layers flattened\n",
    "\n",
    "    def backward(self, doutput):\n",
    "        self.dinputs=doutput.reshape(self.input_shape)\n",
    "    \n",
    "\n",
    "cnn1=CNN_layer(in_channels=1, out_channels=4, kernel_size=3) #28x28-> 26x26\n",
    "relu1=Activation_ReLU()\n",
    "mp1=maxPool(kernel_size=2, stride=2)#26x26-> 13x13\n",
    "cnn2=CNN_layer(in_channels=4, out_channels=8, kernel_size=3)#13x13-> 11x11\n",
    "relu2=Activation_ReLU() \n",
    "mp2=maxPool(kernel_size=2, stride=2)# 11x11-> 5x5\n",
    "f=flatten_layer()\n",
    "nn1=Layer_Dense(n_inputs=8*5*5, n_neurons=10) # 8 channels, 10 feature maps\n",
    "lsfn=Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "52de9dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 8]\n",
      "[5 0]\n"
     ]
    }
   ],
   "source": [
    "def predict(x):\n",
    "    cnn1.forward(x)\n",
    "    relu1.forward(cnn1.output)\n",
    "    mp1.forward(relu1.output)\n",
    "    cnn2.forward(mp1.output)\n",
    "    relu2.forward(cnn2.output)\n",
    "    mp2.forward(relu2.output)\n",
    "    f_output = f.forward(mp2.output)\n",
    "    nn1.forward(f_output)\n",
    "    return np.argmax(nn1.output, axis=1)\n",
    "\n",
    "print(predict(train_data[0:2]))\n",
    "print(train_value[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4d547c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval():\n",
    "     s=0\n",
    "     e=32\n",
    "     predicted=[]\n",
    "     l=len(test_data)\n",
    "     while(e<l):\n",
    "         x=predict(test_data[s:e])\n",
    "         s+=32\n",
    "         e+=32\n",
    "         predicted.extend(x)\n",
    "     if(s+1<l):\n",
    "         x=predict(test_data[s:l])\n",
    "         predicted.extend(x)\n",
    "     predicted = np.array(predicted)\n",
    "     # print(predicted[:5])\n",
    "     # Calculate accuracy using numpy for vectorized comparison\n",
    "     true_labels = test_value\n",
    "     predicted_labels = predicted\n",
    "     accuracy = np.mean(predicted_labels == true_labels)\n",
    "     if(accuracy > 0.9):\n",
    "         print(\"Success!!!\", accuracy * 100, \"%\")\n",
    "     print(\"accuracy:\", accuracy * 100, \"%\")\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b61e0280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'forward'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mstarted...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m e < \u001b[32m60000\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_value\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     s += \u001b[32m32\u001b[39m\n\u001b[32m     38\u001b[39m     e += \u001b[32m32\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(x, value)\u001b[39m\n\u001b[32m      8\u001b[39m relu2.forward(cnn2.output)\n\u001b[32m      9\u001b[39m mp2.forward(relu2.output)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m f_output = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m(mp2.output)\n\u001b[32m     11\u001b[39m nn1.forward(f_output)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: '_io.TextIOWrapper' object has no attribute 'forward'"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train(x, value):\n",
    "    # Forward pass\n",
    "    cnn1.forward(x)\n",
    "    relu1.forward(cnn1.output)\n",
    "    mp1.forward(relu1.output)\n",
    "    cnn2.forward(mp1.output)\n",
    "    relu2.forward(cnn2.output)\n",
    "    mp2.forward(relu2.output)\n",
    "    f_output = f.forward(mp2.output)\n",
    "    nn1.forward(f_output)\n",
    "    # Calculate loss\n",
    "    loss = lsfn.forward(nn1.output, value)\n",
    "    print(\"loss:\", loss)\n",
    "    # Backward pass\n",
    "    lsfn.backward(nn1.output, value)\n",
    "    nn1.backward(lsfn.dinputs)\n",
    "    f.backward(nn1.dinputs)\n",
    "    mp2.backward(f.dinputs)\n",
    "    relu2.backward(mp2.dinputs)\n",
    "    cnn2.backward(relu2.dinputs)\n",
    "    mp1.backward(cnn2.dinputs)\n",
    "    relu1.backward(mp1.dinputs)\n",
    "    cnn1.backward(relu1.dinputs)\n",
    "    # Optimization step\n",
    "    optimizer.update_params(nn1)\n",
    "    cnn1.optimize(learning_rate=0.01)\n",
    "    cnn2.optimize(learning_rate=0.01)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    s = 0\n",
    "    e = 32\n",
    "    print(\"started...\")\n",
    "    while e < 60000:\n",
    "        train(train_data[s:e], train_value[s:e])\n",
    "        s += 32\n",
    "        e += 32\n",
    "    eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bfccc19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters saved to model.txt\n"
     ]
    }
   ],
   "source": [
    "with open('model.txt', 'w') as f:\n",
    "    f.write(\"CNN Layer 1 Weights:\\n\")\n",
    "    f.write(str(cnn1.filters) + \"\\n\")\n",
    "    f.write(\"CNN Layer 1 Biases:\\n\")\n",
    "    f.write(str(cnn1.biases) + \"\\n\")\n",
    "    f.write(\"CNN Layer 2 Weights:\\n\")\n",
    "    f.write(str(cnn2.filters) + \"\\n\")\n",
    "    f.write(\"CNN Layer 2 Biases:\\n\")\n",
    "    f.write(str(cnn2.biases) + \"\\n\")\n",
    "    f.write(\"Dense Layer Weights:\\n\")\n",
    "    f.write(str(nn1.weights) + \"\\n\")\n",
    "    f.write(\"Dense Layer Biases:\\n\")\n",
    "    f.write(str(nn1.biases) + \"\\n\")\n",
    "print(\"Model parameters saved to model.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ed65757f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36meval\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m l=\u001b[38;5;28mlen\u001b[39m(test_data)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m(e<l):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     x=\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     s+=\u001b[32m32\u001b[39m\n\u001b[32m      9\u001b[39m     e+=\u001b[32m32\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mpredict\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      3\u001b[39m relu1.forward(cnn1.output)\n\u001b[32m      4\u001b[39m mp1.forward(relu1.output)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mcnn2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmp1\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m relu2.forward(cnn2.output)\n\u001b[32m      7\u001b[39m mp2.forward(relu2.output)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mCNN_layer.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m in_ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.in_channels):\n\u001b[32m     40\u001b[39m     patch = \u001b[38;5;28minput\u001b[39m[b, in_ch, i:i+\u001b[38;5;28mself\u001b[39m.kernel_size, j:j+\u001b[38;5;28mself\u001b[39m.kernel_size]\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     conv_sum += \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m[\u001b[49m\u001b[43mout_ch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_ch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mself\u001b[39m.output[b, out_ch, i, j] = conv_sum + \u001b[38;5;28mself\u001b[39m.biases[out_ch]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/ML/venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:2333\u001b[39m, in \u001b[36m_sum_dispatcher\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2327\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPassing `min` or `max` keyword argument when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2328\u001b[39m                          \u001b[33m\"\u001b[39m\u001b[33m`a_min` and `a_max` are provided is forbidden.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[33m'\u001b[39m\u001b[33mclip\u001b[39m\u001b[33m'\u001b[39m, a_min, a_max, out=out, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum_dispatcher\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2334\u001b[39m                     initial=\u001b[38;5;28;01mNone\u001b[39;00m, where=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2335\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[32m   2338\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[32m   2339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue,\n\u001b[32m   2340\u001b[39m         initial=np._NoValue, where=np._NoValue):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698cd086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
