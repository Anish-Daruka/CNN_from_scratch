{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"after each layer->\n",
    "    output = sum (inputs * weights) + bias\n",
    "    output = activation(output)\n",
    "   calculating loss fn\n",
    "   backward propagation->calculating gradients\n",
    "   optimization->gradient descent\n",
    "\"\"\"\n",
    "# numpy array->as a vector,python list->need loops\n",
    "hyperparameters_for_cnnlayer = {\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 32,\n",
    "    'kernel':3,\n",
    "    'padding': 0,\n",
    "    'stride': 1\n",
    "}\n",
    "\"\"\"model_structure->\n",
    "28*28->26*26(4 filters)->ReLU->13*13(max_pooling)->11*11(8 filters)->ReLU->5*5(max_pooling)->linear(8*5*5)->10\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ca5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the datasets from mnist custom data in a regular manner\n",
    "train = torchvision.datasets.MNIST(root='./data', train=True, download=False, transform=ToTensor())\n",
    "test = torchvision.datasets.MNIST(root='./data', train=False, download=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7c3b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data = np.array(train.data)/255 # convert to numpy array and normalize\n",
    "print(train_data.shape)\n",
    "print(train_data[1])\n",
    "train_value = np.array(train.targets) # convert to numpy array\n",
    "print(train_value.shape)\n",
    "print(train_value[0])\n",
    "train_classes=classes= np.array(train.classes)\n",
    "print(train_classes)\n",
    "\n",
    "\n",
    "test_data = np.array(test.data)/255 # convert to numpy array and normalize\n",
    "print(test_data.shape)\n",
    "print(test_data[1])\n",
    "test_value = np.array(test.targets) # convert to numpy array\n",
    "print(test_value.shape)\n",
    "print(test_value[0])\n",
    "test_classes = classes = np.array(test.classes)\n",
    "print(test_classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d356ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new axis for features (channels) to train_data and test_data\n",
    "# Shape changes from (N, 28, 28) to (N, 1, 28, 28)\n",
    "train_data = train_data[:, np.newaxis, :, :]\n",
    "test_data = test_data[:, np.newaxis, :, :]\n",
    "print(\"train_data shape:\", train_data.shape)\n",
    "print(\"test_data shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.dot(np.array([2,3,4]),np.array([[1,2,3],[4,5,6],[7,8,9]]).T))\n",
    "a=np.array([[1,2,-3],[4,5,6],[7,8,9]])\n",
    "print(np.maximum(0,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating a neural network layer\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        # print(\"here\")\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        # print(\"here1\")\n",
    "        samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # print(\"y_pred:\",y_pred_clipped)\n",
    "\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # # If labels are one-hot encoded,\n",
    "        # # turn them into discrete values\n",
    "        # if len(y_true.shape) == 2:\n",
    "        #     y_true = np.argmax(y_true, axis=1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings\n",
    "    # Learning rate of 1.0 is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "\n",
    "\n",
    "optimizer=Optimizer_SGD(learning_rate=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc4601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating CNN layer\n",
    "\n",
    "class CNN_layer:\n",
    "    def __init__(self, in_channels=1, out_channels=1, kernel_size=3):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # Initialize filters: [out_channels, in_channels, kernel_size, kernel_size]\n",
    "        # Each output channel has its own set of filters for all input channels\n",
    "        self.filters = 0.1 * np.random.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        \n",
    "        # One bias per output channel\n",
    "        self.biases = 0.1 * np.random.randn(out_channels)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "       \n",
    "        self.input = input  \n",
    "\n",
    "        # Add batch dimension if missing\n",
    "\n",
    "        batch_size, channels, h, w = input.shape\n",
    "        assert channels == self.in_channels, f\"Expected {self.in_channels} channels, got {channels}\"\n",
    "\n",
    "        out_h = h - self.kernel_size + 1\n",
    "        out_w = w - self.kernel_size + 1\n",
    "\n",
    "        # Feature maps for each output channel and batch\n",
    "        self.output = np.zeros((batch_size, self.out_channels, out_h, out_w))\n",
    "\n",
    "        # Apply each output filter for each sample in batch\n",
    "        for b in range(batch_size):\n",
    "            for out_ch in range(self.out_channels):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        conv_sum = 0\n",
    "                        for in_ch in range(self.in_channels):\n",
    "                            patch = input[b, in_ch, i:i+self.kernel_size, j:j+self.kernel_size]\n",
    "                            conv_sum += np.sum(patch * self.filters[out_ch, in_ch])\n",
    "                        self.output[b, out_ch, i, j] = conv_sum + self.biases[out_ch]\n",
    "    \n",
    "    def backward(self, doutput):\n",
    "        input = self.input\n",
    "        batch_size, channels, h, w = input.shape\n",
    "        out_h = h - self.kernel_size + 1\n",
    "        out_w = w - self.kernel_size + 1\n",
    "\n",
    "        self.dfilters = np.zeros_like(self.filters)\n",
    "        self.dbiases = np.zeros_like(self.biases)\n",
    "        self.dinputs = np.zeros_like(input)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for out_ch in range(self.out_channels):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        for in_ch in range(self.in_channels):\n",
    "                            patch = input[b, in_ch, i:i+self.kernel_size, j:j+self.kernel_size]\n",
    "                            self.dfilters[out_ch, in_ch] += doutput[b, out_ch, i, j] * patch\n",
    "                            self.dinputs[b, in_ch, i:i+self.kernel_size, j:j+self.kernel_size] += doutput[b, out_ch, i, j] * self.filters[out_ch, in_ch]\n",
    "                            self.dbiases[out_ch] += doutput[b, out_ch, i, j]\n",
    "\n",
    "        # Remove batch dimension if input was single sample\n",
    "        if len(self.input.shape) == 3:\n",
    "            self.dinputs = self.dinputs[0]\n",
    "    \n",
    "    def optimize(self,learning_rate=0.01):\n",
    "        # Update filters and biases using gradients computed in backward pass\n",
    "        self.filters -= learning_rate * self.dfilters\n",
    "        self.biases -= learning_rate * self.dbiases\n",
    "\n",
    "\n",
    "\n",
    "class maxPool:\n",
    "    def __init__(self, kernel_size=2, stride=2):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        batch_size, channels, h, w = x.shape\n",
    "        out_h = (h - self.kernel_size) // self.stride + 1\n",
    "        out_w = (w - self.kernel_size) // self.stride + 1\n",
    "        self.output = np.zeros((batch_size, channels, out_h, out_w))\n",
    "        self.max_indices = np.zeros_like(self.output, dtype=int)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "                        patch = x[b, c, h_start:h_end, w_start:w_end]\n",
    "                        self.output[b, c, i, j] = np.max(patch)\n",
    "                        self.max_indices[b, c, i, j] = np.argmax(patch)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, doutput):\n",
    "        \"\"\"\n",
    "        doutput: gradient of loss w.r.t. output, same shape as self.output\n",
    "        \"\"\"\n",
    "        batch_size, channels, h, w = self.input.shape\n",
    "        self.dinputs = np.zeros_like(self.input)\n",
    "        out_h, out_w = doutput.shape[2], doutput.shape[3]\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "                        patch = self.input[b, c, h_start:h_end, w_start:w_end]\n",
    "                        max_idx = self.max_indices[b, c, i, j]\n",
    "                        idx = np.unravel_index(max_idx, patch.shape)\n",
    "                        self.dinputs[b, c, h_start:h_end, w_start:w_end][idx] += doutput[b, c, i, j]\n",
    "\n",
    "\n",
    "class flatten_layer:\n",
    "    def forward(self, x):\n",
    "        self.input_shape = x.shape\n",
    "        return x.reshape(x.shape[0], -1)    #batch_size, then all rest of layers flattened\n",
    "\n",
    "    def backward(self, doutput):\n",
    "        self.dinputs=doutput.reshape(self.input_shape)\n",
    "    \n",
    "\n",
    "cnn1=CNN_layer(in_channels=1, out_channels=4, kernel_size=3) #28x28-> 26x26\n",
    "relu1=Activation_ReLU()\n",
    "mp1=maxPool(kernel_size=2, stride=2)#26x26-> 13x13\n",
    "cnn2=CNN_layer(in_channels=4, out_channels=8, kernel_size=3)#13x13-> 11x11\n",
    "relu2=Activation_ReLU() \n",
    "mp2=maxPool(kernel_size=2, stride=2)# 11x11-> 5x5\n",
    "f=flatten_layer()\n",
    "nn1=Layer_Dense(n_inputs=8*5*5, n_neurons=10) # 8 channels, 10 feature maps\n",
    "lsfn=Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    cnn1.forward(x)\n",
    "    relu1.forward(cnn1.output)\n",
    "    mp1.forward(relu1.output)\n",
    "    cnn2.forward(mp1.output)\n",
    "    relu2.forward(cnn2.output)\n",
    "    mp2.forward(relu2.output)\n",
    "    f_output = f.forward(mp2.output)\n",
    "    nn1.forward(f_output)\n",
    "    return np.argmax(nn1.output, axis=1)\n",
    "\n",
    "print(predict(train_data[0:2]))\n",
    "print(train_value[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d547c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval():\n",
    "     s=0\n",
    "     e=32\n",
    "     predicted=[]\n",
    "     l=len(test_data)\n",
    "     while(e<l):\n",
    "         x=predict(test_data[s:e])\n",
    "         s+=32\n",
    "         e+=32\n",
    "         predicted.extend(x)\n",
    "     if(s+1<l):\n",
    "         x=predict(test_data[s:l])\n",
    "         predicted.extend(x)\n",
    "     predicted = np.array(predicted)\n",
    "     # print(predicted[:5])\n",
    "     # Calculate accuracy using numpy for vectorized comparison\n",
    "     true_labels = test_value\n",
    "     predicted_labels = predicted\n",
    "     accuracy = np.mean(predicted_labels == true_labels)\n",
    "     if(accuracy > 0.9):\n",
    "         print(\"Success!!!\", accuracy * 100, \"%\")\n",
    "     print(\"accuracy:\", accuracy * 100, \"%\")\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(x, value):\n",
    "    # Forward pass\n",
    "    cnn1.forward(x)\n",
    "    relu1.forward(cnn1.output)\n",
    "    mp1.forward(relu1.output)\n",
    "    cnn2.forward(mp1.output)\n",
    "    relu2.forward(cnn2.output)\n",
    "    mp2.forward(relu2.output)\n",
    "    f_output = f.forward(mp2.output)\n",
    "    nn1.forward(f_output)\n",
    "    # Calculate loss\n",
    "    loss = lsfn.forward(nn1.output, value)\n",
    "    print(\"loss:\", loss)\n",
    "    # Backward pass\n",
    "    lsfn.backward(nn1.output, value)\n",
    "    nn1.backward(lsfn.dinputs)\n",
    "    f.backward(nn1.dinputs)\n",
    "    mp2.backward(f.dinputs)\n",
    "    relu2.backward(mp2.dinputs)\n",
    "    cnn2.backward(relu2.dinputs)\n",
    "    mp1.backward(cnn2.dinputs)\n",
    "    relu1.backward(mp1.dinputs)\n",
    "    cnn1.backward(relu1.dinputs)\n",
    "    # Optimization step\n",
    "    optimizer.update_params(nn1)\n",
    "    cnn1.optimize(learning_rate=0.01)\n",
    "    cnn2.optimize(learning_rate=0.01)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    s = 0\n",
    "    e = 32\n",
    "    print(\"started...\")\n",
    "    while e < 60000:\n",
    "        train(train_data[s:e], train_value[s:e])\n",
    "        s += 32\n",
    "        e += 32\n",
    "    eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a894b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
